    unikernel的一个应用是clickOS，它在动态实例化完全可编程网络和快速移动middlebox中可以起到很好的效果。
    clickos作为一款基于xen的小型虚拟机，可以运行各种各样的middlebox。由于其轻量的特性（运行时只有5mb），它能在极短的时间内实例化（大约30ms）并且可以在低成本商用服务器上在运行128个vm的同时填充一个10gb的管道。
    事实上，在现在的互联网中，middlebox有着普遍的应用，但是实现起来都很昂贵。一个好的的软件和middlebox之间的接口应该满足以下要求：快速实例化，占地面积小，隔离，表现，灵活性。也就是兼具安全与性能与成本的要求。虽然默认的解决方案是运行middlebox虚拟机，但是内存占用量过大；Click模型有性能和灵活性上的优点但是在隔离上的却做的不尽人意，无法达到安全上的要求,同时，Click与linux内核的耦合并不是根本性的，所以我们需要一个创新来打破它。
    而ClickOS这个基于XEN的虚拟机恰好能很好的满足我们在效率，安全，成本上的需求。
##CLICKOS结构
    为了运行Click，用户实际上提供配置指定互联元素图表的文本文件，一旦运行，它们就可以访问内部读写处理程序，并且可以在运行的时候改变元素的状态。Click依赖于/proc文件系统或socket来提供这些机制，但是clickOS中并不存在这些，我们必须提供一个等效的方式来实施他们。
    负责处理这些操作的ClockOS控制平台由三个部分组成：首先，一个基于c语言的CLI来管理，创建和删除ClickOS虚拟机。每当虚拟机被创建时，一个MiniOS控制线程就会被创建（控制平台的第二部分）。这个线程会添加一个向XENstore（一个类/proc的在所有正在运行的虚拟机之间共享的数据库）的入口。控制线程随后就会监控这些入口的变化。当一个Click配置字符串被写入时，它就会开始创建一个新线程并且在其中运行一个Click实例化，这意味着一个ClickOS中可以运行多个Click实例。
    ClickOS控制平台的第三部分包含了一个叫做ClickOSControl的新的Click元素。CLI可以向用户提供一个可以向元素管理器读写的接口。所有这些操作都是在从之线程的管理之下执行的。
##创建ClickOS
    XEN被分为一个被称为dom（）的特权域和一个用户域即用户的虚拟机。
    为了得到更好的表现，Click应该在操作系统内核中直接运行但是这很容易就会引起整个操作系统的冲突，所以Click就只能被运行在虚拟机上。但是即使是在今天，使用完整的虚拟机来运行Click也是一个相重的负担，还会产生相当大的内存占用和很长的启动时间。
    这时我们就注意到实际上很多linux内核提供的服务都是运行ClickOS不需要的。首先，一个ClickOS会运行一个面向单个用户的配置程序，所以并不需要多用户支持。事实上，用户空间程序也是不需要的，移除user-space/kernel空间能提高表现并且简化内核也是显而易见的了。
    同时，Click并不需要复数的内存地址空间，由于所有元素都可以互相传递指针来完成它们的功能，所以一个单独的配置只需要在一个相同的地址空间中运行。这也表示复线程功能的支持也是不需要的。不过线程倒是需要像平行线一样各自执行不同的进程。
    至于I/O方面Click需要访问网络接口，这个很容易就能用一个通用驱动程序支持，但是这样会带来提供不同硬件驱动的复杂性，事实上移除大部分的I/O支持弃用了大部分的Linux内核。最后TCP/IP可能需要网络堆栈连接
    事实证明，Xen源自MiniOS这个最小化的，半虚拟化的操作系统，它提供所有Click需要的功能，并且不会加入传统操作系统中存在的冗余部分。就结果来看，ClickOS是Click和miniOS的结合。
    MiniOS建立了在Xen中操作的基础，同时，它还具有一个单独地址空间，避免了内核与用户的分离，并运行一个合作调度程序，减少了转换过程中的花费。
    而为了建立Click与MiniOS的连接，我们首先需要一个不依赖于linux的cpp编译器。为了实现它，建立了一个新的工具链，它使用独立于平台的newlibc library而不是glibc。
    同时，还调整了Click的一些部分来让它能够在MiniOS环境中工作而不是向Linux这样的完整的操作系统。
##Click的网络
    Xen拥有一个独立的网络驱动模式，这种模式可以使虚拟机使用硬件驱动而不需要它们自己管理这些驱动。
    在经典的Xen环境下，网卡通过Linux bridge模块与叫做vif的虚拟网络设备连接；而在后来的Xen版本下打开xSwitch。当一个包被接收后，它就被送到vif中。相应设备会将这个包放入网络后端驱动的队列中。之后，其中一个驱动线程会接受这个包并且将它放入共享的环域中，并通知进程中的网络前端。被发送出去的包则是按照相反的路径被送出。
    为了连接网络前端驱动，建立两个Click元素，fromnetfront和tonetfront。前者负责初始化驱动而每次它被开启就会恢复网络前端包的burst数。而后者就很直接了，它只是简单的调用网络前端的运输功能。
    不经过优化的话，这个网路路径的表现会很差，差到最大只能达到8kp/s，而即使是使用基于Linux的虚拟机也能达到2.9Gb/s。为了让它能达到我们网卡所能支持的最大10Gb，我们需要做数项改进。
    首先，我们介绍两个用来优化网络前端的机能：首先我们将驱动接收包的函数改成从运行Click的MiniOS线程中拉取包；第二，重新利用被授予的接受缓冲区并且避免他们随着网络运行的寿命终止。
    第二个瓶颈是网络后端和API。优化这些需要彻底分析Xen的底层，这里就不在赘述细节，主要是在NIC和vm之间提供一个更加直接的网络路径以获得更好的表现。
    最后一个瓶颈就是软件之间的切换，它的性能非常差，所以用VALE（一个基于netmap的切换机制）来替换它并且将它改编以至于能够和ClickOS的网络前端驱动相连。
    




















